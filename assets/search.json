

[
  
  
    
    
      {
        "title": "Description of an Alembic",
        "excerpt": "The complete distilling apparatus consists of three parts: the “cucurbit” (Arabic ḳarʿa, Greek βίκος), the still pot containing the liquid to be distilled, which is heated by a flame; the “head” or “cap” (Arabic anbiḳ, Greek ἄμβιξ) which fits over the mouth of the cucurbit to receive the vapors, with an attached downward-sloping “tube” (Greek σωλήν), leading to the “receiver” (Arabic ḳābila, Greek ἄγγος or φιάλη) container.\n\n",
        "content": "The complete distilling apparatus consists of three parts: the “cucurbit” (Arabic ḳarʿa, Greek βίκος), the still pot containing the liquid to be distilled, which is heated by a flame; the “head” or “cap” (Arabic anbiḳ, Greek ἄμβιξ) which fits over the mouth of the cucurbit to receive the vapors, with an attached downward-sloping “tube” (Greek σωλήν), leading to the “receiver” (Arabic ḳābila, Greek ἄγγος or φιάλη) container.\n\n\n\nRetorts have the “cap” and the “cucurbit” made into one. The anbik is also called the raʾs (head) of the cucurbit. The liquid in the cucurbit is heated or boiled; the vapour rises into the anbik, where it cools by contact with the walls and condenses, running down the spout into the receiver. A modern descendant of the alembic is the pot still, used to produce distilled beverages.\n\nOriginally from Alembic - Wikipedia\n",
        "url": "/general/external%20sources/2016/08/27/example-post-one/"
      },
    
      {
        "title": "History of the Alembic",
        "excerpt": "Dioscorides’ ambix (described in his De materia medica) is a helmet-shaped lid for gathering condensed mercury. For Athenaeus (~ 225 C.E.) it is a bottle or flask. For later chemists it denotes various parts of crude distillation devices.\n\n",
        "content": "Dioscorides’ ambix (described in his De materia medica) is a helmet-shaped lid for gathering condensed mercury. For Athenaeus (~ 225 C.E.) it is a bottle or flask. For later chemists it denotes various parts of crude distillation devices.\n\n\n\nAlembic drawings appear in works of Cleopatra the Alchemist, Synesius, and Zosimos of Panopolis. There were alembics with two (dibikos) and three (tribikos) receivers.[4] According to Zosimos of Panopolis, the alembic was invented by Mary the Jewess.[5]\n\nThe anbik is described by Ibn al-Awwam in his Kitab al-Filaha (Book of Agriculture), where he explains how rose-water is distilled. Amongst others, it is mentioned in the Mafatih al-Ulum (Key of Sciences) of Khwarizmi and the Kitab al-Asrar (Book of Secrets) of Al-Razi. Some illustrations occur in the Latin translations of works which are attributed to Geber.[2]\n\nOriginally from Alembic - Wikipedia\n",
        "url": "/history/external%20sources/2016/08/28/example-post-two/"
      },
    
      {
        "title": "Description of a Pot Still",
        "excerpt": "A pot still is a type of still used in distilling spirits such as whisky or brandy. Heat is applied directly to the pot containing the wash (for whisky) or wine (for brandy).\n",
        "content": "A pot still is a type of still used in distilling spirits such as whisky or brandy. Heat is applied directly to the pot containing the wash (for whisky) or wine (for brandy). This is called a batch distillation (as opposed to a continuous distillation).\n\nAt standard atmospheric pressure, alcohol boils at 78 °C (172 °F), while water boils at 100 °C (212 °F). During distillation, the vapour contains more alcohol than the liquid. When the vapours are condensed, the resulting liquid contains a higher concentration of alcohol. In the pot still, the alcohol and water vapour combine with esters and flow from the still through the condensing coil. There they condense into the first distillation liquid, the so-called “low wines”. The low wines have a strength of about 25–35% alcohol by volume, and flow into a second still. It is then distilled a second time to produce the colourless spirit, collected at about 70% alcohol by volume. Colour is added through maturation in an oak aging barrel, and develops over time.\n\nThe modern pot still is a descendant of the alembic, an earlier distillation device.\n",
        "url": "/general/2016/08/29/example-post-three/"
      },
    
  
  
  
  {
    "title": "Committees",
    "excerpt": "s\n",
    "content": "Conference Chairs\n\n\n  \n    \n  \n  \n    \n      Warren Gross\n    \n    \n      McGill University\n    \n  \n\n\n  \n    \n  \n  \n    \n      Vahid Partovi Nia\n    \n    \n      Huawei Noah's Ark Lab, Polytechnique Montréal\n    \n  \n\n\nOrganizing Committee\n\n\n  \n    \n  \n  \n    \n      Xi Chen\n    \n    \n      McGill University\n    \n  \n\n\n  \n    \n  \n  \n    \n      James Clark\n    \n    \n      McGill University\n    \n  \n\n\n  \n    \n  \n  \n    \n      Negin Firouzian\n    \n    \n      McGill University\n    \n  \n\n\n  \n    \n  \n  \n    \n      Ghouti Boukli Hacene\n    \n    \n      Sony\n    \n  \n\n\n  \n    \n  \n  \n    \n      Mehdi Rezagholizadeh\n    \n    \n      Huawei Noah's Ark Lab\n    \n  \n\n\n  \n    \n  \n  \n    \n      Brett Meyer\n    \n    \n      McGill University\n    \n  \n\n\n  \n    \n  \n  \n    \n      Mohammadreza Tayaranian\n    \n    \n      McGill University\n    \n  \n\n\nTechnical Committee\n\n\n  \n    \n  \n  \n    \n      Masoud Asgharian\n    \n    \n      McGill University\n    \n  \n\n\n  \n    \n  \n  \n    \n      Charles Audet\n    \n    \n      GERAD and Polytechnique Montréal\n    \n  \n\n\n  \n    \n  \n  \n    \n      Tiago Falk\n    \n    \n      INRS\n    \n  \n\n\n  \n    \n  \n  \n    \n      Ali Ghodsi\n    \n    \n      University of Waterloo\n    \n  \n\n\n  \n    \n  \n  \n    \n      Sebastien Le Digabel\n    \n    \n      Polytechnique Montreal\n    \n  \n\n\n  \n    \n  \n  \n    \n      Alejandro Murua\n    \n    \n      Université de Montréal\n    \n  \n\n\n  \n    \n  \n  \n    \n      Dominique Orban\n    \n    \n      GERAD and Polytechnique Montréal\n    \n  \n\n\n  \n    \n  \n  \n    \n      Yvon Savaria\n    \n    \n      Polytechnique Montreal\n    \n  \n\n\n  \n    \n  \n  \n    \n      Yaoliang Yu\n    \n    \n      University of Waterloo\n    \n  \n\n\n  \n    \n  \n  \n    \n      Chao Xing\n    \n    \n      Huawei Noah's Ark Lab\n    \n  \n\n",
    "url": "/Committees/"
  },
  
  {
    "title": "Full Proceedings",
    "excerpt": "s\n",
    "content": "\n  \n    Proceedings of the Edge Intelligence Workshop 2020, Montréal, Canada, March 2–3, 2020. C. Audet, S. Le Digabel, A. Lodi, D. Orban and V. Partovi Nia, (Eds.) (April 2020). Technical reports, Les Cahiers du GERAD G–2020–23, GERAD, HEC Montréal, Canada.\n  \n  \n    Proceedings of the Edge Intelligence Workshop 2022, Montréal, Canada will appear soon.\n  \n\n",
    "url": "/Proceedings/"
  },
  
  {
    "title": "Registration",
    "excerpt": "s\n",
    "content": "Use this link and choose start registration.\n\n\n  \n    Regular fee: 500 CAD\n  \n  \n    Student fee: 100 CAD\n  \n\n",
    "url": "/Registration/"
  },
  
  {
    "title": "Scientific Program",
    "excerpt": "s\n",
    "content": "\n\n\n  \n      Monday September 19 \n  \n  \n      08:00-08:30 \n    Breakfast (Registration and Poster setup)\n  \n\n  \n      08:30-09:30 \n    Diana Marculescu\n    Keynote: \"When Sustainability Meets Machine Learning: Efficient Learning from Cloud to Edge\"\n     \n  \n  \n      09:30-10:00 \n    James Clark\n    What is lost when networks are compressed?\n  \n    \n      10:00-10:30 \n    Coffee Break and Posters\n  \n   \n      10:30-11:00 \n    Vahid Partovi Nia\n    Edge implementation of deep models\n     \n  \n\n   \n      11:00-11:30 \n    Mark Coates\n    Efficient Bayesian Network Architecture Search for Graph Neural Networks\n  \n\n   \n      11:30-12:00 \n    Ehsan Saboori\n    Running 2 bit quantized CNN models on ARM CPUs\n  \n\n  \n      12:00-13:00 \n    Lunch\n  \n   \n      13:00-15:00 \n    Evgeni Gousev \n    Keynote: \"tinyML: ultimate energy efficient machine learning solution for edgeAI\" \n     \n  \n\n   \n      14:00-14:30 \n    Muthucumaru Maheswaran\n    JAMScript: A Programming Language for Edge Oriented Mobile Internet of Things\n  \n\n   \n      14:30-15:00 \n    Shahrokh Valaee\n    Cooperative Location Estimation using Federated Learning\n  \n\n  \n      15:00-15:30 \n    Coffee Break and Posters\n  \n\n   \n      15:30-16:00 \n    Rachel E. Bouserhal\n    Hearables and their potential as a tool for early disease detection\n     \n  \n\n   \n      16:00-16:30 \n    Dounia Lakhmiri\n    A Stochastic Proximal Method for Nonsmooth Regularized Finite Sum Optimization\n  \n\n   \n      16:30-17:00 \n    Masoud Asgharian\n    TBD\n  \n\n   \n      17:00-17:30 \n    Michael Rabbat\n    Asynchronous Federated Learning at Scale\n  \n\n\n\n\n\n\n  \n      Tuesday September 20 \n  \n  \n      08:00-08:30 \n    Breakfast (Registration and Poster setup)\n  \n\n  \n      08:30-09:30 \n    Wen Tong\n    Keynote: Machine Learning Based Post-Shannon Cognition Communications\n    \n  \n  \n      09:30-10:00 \n    Brett Meyer\n    Transforming Intelligence for the Edge:\nChallenges and Opportunities in Modeling, Optimization, and Deployment\n  \n    \n      10:00-10:30 \n    Coffee Break and Posters\n  \n   \n      10:30-11:00 \n    Yunaho Yu\n    Challenges for Edge Device Machine Learning Platform\n    \n  \n\n   \n      11:00-11:30 \n    Naoya Onizawa\n    Fast-Converging Simulated Annealing for Ising Models Based on Integral Stochastic Computing\n  \n\n   \n      11:30-12:00 \n    Ghouthi Boukli Hacene\n    DNN Quantization and acceleration for training and inference\n  \n\n  \n      12:00-13:00 \n    Lunch\n  \n   \n      13:00-14:00 \n    Song Han\n    Keynote: Efficient AI Computing with Sparsity\n     \n  \n\n   \n      14:00-14:30 \n    Christophe Dubach\n    Very High-Level Synthesis of Neural Networks Accelerators for FPGAs\n  \n\n   \n      14:30-15:00 \n    Francois Leduc-Primeau\n    Building Energy-Efficient AI Chips by Exploiting Energy-Reliability Tradeoffs\n  \n\n  \n      15:00-15:30 \n    Coffee Break and Posters\n  \n\n   \n      15:30-16:00 \n    Yvon Savaria\n    Applications of Edge Intelligence, Applications, Lessons Learned and Platforms\n     \n  \n\n   \n      16:00-16:30 \n    Andreas Moshovos\n    TBD\n  \n\n   \n      16:30-17:00 \n    Pascal Poupart\n    Uncertainty Aware Federated Learning\n  \n\n   \n      17:00-17:30 \n    Sarath Chandar\n    TBD\n  \n\n\n\n\n\n\n   \n     Board Number \n     Poster Title  \n  \n\n   \n    Monday September 19 \n  \n\n   \n    1\n    Weighted Group L0-norm Constraint for Sparse Training \n  \n\n   \n    2\n    NAS plus Pipeline for High Throughput Edge Inference BERT \n  \n\n   \n    3\n    Generalizing ProxConnect on Vision Transformer Binarization \n  \n\n   \n    4\n    An Exploration into the Performance of Unsupervised Cross-Task Speech Representations for ''In the Wild'' Edge Applications \n  \n\n   \n    5\n    GHN-Q: Parameter Prediction for Unseen Quantized Convolutional Architectures via Graph Hypernetworks\n  \n\n   \n    6\n    A Decomposition Method Supporting Many Factorization Structures\n  \n\n   \n    7\n    Retention of Domain Adaptability in Compressed Neural Networks  \n  \n\n   \n    8\n    Sharpness-Aware Training for Accurate Inference on Noisy DNN Accelerators \n  \n\n   \n    9\n    On the Importance of Integrating Curriculum Design for Teacher Assistant-based Knowledge Distillation\n  \n\n   \n    10\n    Towards Finding Efficient Students via Blockwise Neural Architecture Search and Knowledge Distillation\n  \n\n   \n    11\n    Quasi-convex floating points optmization \n  \n\n   \n    12\n    Standard Deviation-Based Quantization for Deep Neural Networks \n  \n\n   \n    13\n    S^3 Sign-Sparse-Shift Reparametrization for Effective Training of Low-bit Shift Networks\n  \n\n   \n    14\n    Inspecting the Role of Pretrained Transformers in Federated Learning \n  \n\n  \n    15\n    Quantized One-dimensional Stacked CNN for Seizure Forecasting with Wearables \n  \n\n   \n    16\n    BERT Inference Energy Predictor for Efficient Hardware-aware NAS \n  \n\n   \n    17\n    Speeding up Resnet Architecture with Layers Targeted Low Rank Decomposition\n  \n\n   \n    18\n    Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization \n  \n\n  \n\n\n\n\n  \n     Tuesday September 20  \n  \n\n   \n    1\n    Limited-Memory Stochastic Partitioned Quasi-Newton Training\n  \n\n   \n    2\n    A Short Study on Compressing Decoder-Based Language Models\n  \n\n   \n    3\n    Faster Attention Is What You Need: A Fast Self-Attention Neural Network Backbone Architecture for the Edge via Double-Condensing Attention Condensers\n  \n\n   \n    4\n    Quadratic Regularization Optimizer in Low Precision for Deep Neural Networks: Implementation and Numerical Experience\n  \n\n   \n    5\n    Gradient Distribution Theory for Exploding and Vanishing Gradient Problem\n  \n\n   \n    6\n    Mixed representation integer fine-tuning of transformer-based models\n  \n\n   \n    7\n    How Robust is Robust wav2vec 2.0 for Edge Applications?: An Exploration into the Effects of Quantization and Model Pruning on “In-the-Wild” Speech Recognition\n  \n\n   \n    8\n    ARMCL BERT: Novel Quantizable BERT Implementation for ARM SoCs \n  \n\n   \n    9\n    Kronecker Decomposition for GPT Compression\n  \n\n   \n    10\n    Dyadic Integer Only BERT\n  \n\n   \n    11\n    Learning Gaussian Restricted Boltzmann Machine using tensorial decompositions\n  \n\n   \n    12\n    Persona Controlled Dialogue Prompting \n  \n\n   \n    13\n    Toward Training Neural Networks with a Multi-Precision Quadratic Regularization Algorithm\n  \n\n   \n    14\n    iRNN: Integer-only Recurrent Neural Network \n  \n\n  \n    15\n    Latency and Accuracy Predictors for Efficient BERT Hardware-aware NAS \n  \n\n   \n    16\n    Rational SoftMax  \n  \n\n   \n    17\n    Partially-Random Initialization: A Smoking Gun for Binarization Hypothesis of BERT\n  \n\n\n\n",
    "url": "/ScientificProgram/"
  },
  
  {
    "title": "Registration",
    "excerpt": "s\n",
    "content": "Use this link to make a new submission.\n",
    "url": "/Submission/"
  },
  
  {
    "title": "Categories",
    "excerpt": "Category index\n",
    "content": "\n",
    "url": "/categories/"
  },
  
  {
    "title": "Search",
    "excerpt": "Search for a page or post you’re looking for\n",
    "content": "{% include site-search.html %}\n",
    "url": "/search/"
  }
  
]

